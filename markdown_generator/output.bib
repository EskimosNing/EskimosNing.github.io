@inproceedings{10.5555/3635637.3663105,author={Chen, Siqi and Zhao, Jianing and Zhao, Kai and Weiss, Gerhard and Zhang, Fengyun and Su, Ran and Dong, Yang and Li, Daqian and Lei, Kaiyou},title={ANOTO: Improving Automated Negotiation via Offline-to-Online Reinforcement Learning},year={2024},isbn={9798400704864},publisher={International Foundation for Autonomous Agents and Multiagent Systems},address={Richland, SC},abstract={Automated negotiation is a crucial component for establishing cooperation and collaboration within multi-agent systems.While reinforcement learning (RL)-based negotiating agents have achieved remarkable success in various scenarios, they still face limitations due to certain assumptions on which they are based.In this work, we proposes a novel approach called ANOTO to improve the negotiating agents' ability via offline-to-online RL.ANOTO enables a negotiating agent (1) to communicate with opponents using an end-to-end strategy that covers all negotiation actions, (2) to learn negotiation strategies from historical offline data without requiring active interactions, and (3) to enhance the optimization process during the online phase, facilitating rapid and stable performance improvements for the learned offline strategies.Experimental results, based on a number of negotiation scenarios and recent winning agents from the Automated Negotiating Agents Competitions (ANAC), are provided.},booktitle={Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},pages={2195–2197},numpages={3},keywords={automated negotiation, e-commence, reinforcement learning},location={Auckland, New Zealand},series={AAMAS '24}}

@InProceedings{pmlr-v216-chen23c,title={An effective negotiating agent framework based on deep offline reinforcement learning},author={Chen, Siqi and Zhao, Jianing and Weiss, Gerhard and Su, Ran and Lei, Kaiyou},booktitle={Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},pages={324--335},year={2023},editor={Evans, Robin J. and Shpitser, Ilya},volume={216},series={Proceedings of Machine Learning Research},month={07},publisher={PMLR},pdf={https://proceedings.mlr.press/v216/chen23c/chen23c.pdf},url={https://proceedings.mlr.press/v216/chen23c.html},abstract={Learning is crucial for automated negotiation, and recent years have witnessed a remarkable achievement in application of reinforcement learning (RL) for various negotiation tasks. Conventional RL methods focus generally on learning from active interactions with opposing negotiators. However, collecting online data is expensive in many realistic negotiation scenarios. While previous studies partially mitigate this problem through the use of opponent simulators (i.e., agents following known strategies), in reality it is usually hard to fully capture an opponent’s negotiation strategy. Moreover, a further challenge lies in an agent’s capability of adapting to dynamic variations of an opponent’s preferences or strategies, which may happen from time to time for different reasons in subsequent negotiations. In response to these challenges, this article proposes a novel Deep Offline Reinforcement learning Negotiating Agent framework that allows to learn an effective strategy using previously collected negotiation datasets without requiring interaction with an opponent. This is in contrast to existing RL-based negotiation approaches that all rely on active interaction with opponents. Furthermore, the strategy fine-tuning mechanism is included to adjust the learned strategy in response to the preferences or strategy changes of the opponent. The performance of the proposed framework is evaluated based on a diverse set of state-of-the-art baselines under different settings. Experimental results show that the framework allows to learn effective strategies exclusively with offline datasets, and is also capable of effectively adapting to changes of an opponent’s preferences or strategy.}}

@InProceedings{10.1007/978-3-031-33377-4_30,
author="Chen, Siqi
and Yang, Tianpei
and You, Heng
and Zhao, Jianing
and Hao, Jianye
and Weiss, Gerhard",
editor="Kashima, Hisashi
and Ide, Tsuyoshi
and Peng, Wen-Chih",
title="Transfer Reinforcement Learning Based Negotiating Agent Framework",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="386--397",
abstract="While achieving tremendous success, there is still a major issue standing out in the domain of automated negotiation: it is inefficient for a negotiating agent to learn a strategy from scratch when being faced with an unknown opponent. Transfer learning can alleviate this problem by utilizing the knowledge of previously learned policies to accelerate the current task learning. This work presents a novel Transfer Learning based Negotiating Agent (TLNAgent) framework that allows a negotiating agent to transfer previous knowledge from source strategies optimized by deep reinforcement learning, to boost its performance in new tasks. TLNAgent comprises three key components: the negotiation module, the adaptation module and the transfer module. To be specific, the negotiation module is responsible for interacting with the other agent during negotiation. The adaptation module measures the helpfulness of each source policy based on a fusion of two selection mechanisms. The transfer module is based on lateral connections between source and target networks and accelerates the agent's training by transferring knowledge from the selected source strategy. Our comprehensive experiments clearly demonstrate that TL is effective in the context of automated negotiation, and TLNAgent outperforms state-of-the-art Automated Negotiating Agents Competition (ANAC) negotiating agents in various domains.",
isbn="978-3-031-33377-4"
}


